<!DOCTYPE html>
<html>
	<head>
		<link rel="stylesheet" type="text/css" href="../css/new/style.css" />
		<link rel="preconnect" href="https://fonts.googleapis.com" />
		<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
		<link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;600;900&display=swap" rel="stylesheet" />

		<title>Papers</title>
		<style></style>
	</head>

	<body>
		<a href=https://arxiv.org/pdf/2101.01321> <h1>I-BERT: Integer-only BERT Quantization </h1> </a>
		<h2> Kim et al, 2021 </h2>
		<h4>June 5th, 2023 </h4>
		<br>
		<br>
		<h3>Summary</h3>
		<ul>
			<li> Most "integer only" quantization of NN typically still do non-linear operations in FP</li>
			<li> By doing clever polynomial approximations of GELU, Softmax, and squart root, they were able to implement BERT</li>
			<li> Due to the simplicity of integer only operations, I-BERT preformed better than BERT</li>
			<li> Not only did they see a reduction in latency, they also saw an increase in accuracy</li>
		</ul>
		<h3>Thoughts</h3>
		<ul>
			<li>The approximations for non-linear functions were worse than the floating part counterpart, however ablation studies showed that the model actually preformed better with the integer only version. The paper does not explore this. </li>
			<li> The fact that integer approximations did not significantly reduce preformance implies that the percision of float point arithmetic is not strictly necessary in a transformer model</li>
			<li> This paper suggests that any hardware-conscience implementation of BERT (or an LLM in general) should not attempt to do a full FP implementation</li>
		</ul>
	</body>
</html>
